{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import classes\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and parse the data file\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "np.unique(iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "# a random permutation, to split the data randomly\n",
    "a = np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "# print indices\n",
    "# print np.unique(indices)\n",
    "\n",
    "#[:-10] : elements from 0 to the length - 11\n",
    "#[10:] : elements from 11 to the length - 1\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_y_test = iris_y[indices[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a k-nearest-neighbor model\n",
    "# create and fit a k-nearest-neighbor classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# k = 5 par d√©faut\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train,iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model on test instances and compute test error\n",
    "from sklearn.metrics import accuracy_score\n",
    "knn.predict(iris_X_test)\n",
    "accuracy_score(iris_y_test,knn.predict(iris_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of this classifier : 0.9\n",
      "error of this classifier : 0.1\n",
      "the optimal keys : [8, 9, 10, 11, 12, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92]\n"
     ]
    }
   ],
   "source": [
    "# 1/ write a jupter notebook with the following tasks\n",
    "# 1) write error of the classifier\n",
    "correct = accuracy_score(iris_y_test,knn.predict(iris_X_test))\n",
    "error = 1 - correct\n",
    "print \"accuracy of this classifier : {0}\".format(correct)\n",
    "print \"error of this classifier : {0}\".format(error)\n",
    "\n",
    "\n",
    "# 2) what is the optimal parameter k of the k'nearest-neighbor classifier for this dataset ?\n",
    "correct_tab = dict()\n",
    "for i in range(1,len(iris_X_train)+1) :\n",
    "    knn_test = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_test.fit(iris_X_train, iris_y_train)\n",
    "    knn_test.predict(iris_X_test)\n",
    "    correct_test = accuracy_score(iris_y_test,knn_test.predict(iris_X_test))\n",
    "    correct_tab[i] = correct_test\n",
    "maxx = max(correct_tab.values())    \n",
    "keys = [x for x,y in correct_tab.items() if y ==maxx] \n",
    "print \"the optimal keys : {0}\".format(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct score of the svc classifier : 0.9\n",
      "correct score of the bayes classifier : 0.9\n",
      "correct score of the linear regression : 0.904727027501\n",
      "result of cross-validation of knn : 0.986928104575\n",
      "result of cross-validation of svm : 0.973447712418\n",
      "result of cross-validation of bayes : 0.934232026144\n",
      "correct score of cross-validation of regression : 0.3228834276\n",
      "the best classifiers is : knn, the result of this classifiers is : 0.986928104575\n"
     ]
    }
   ],
   "source": [
    "# 2/ write a jupyter notebook with the following tasks\n",
    "# with the iris dataset\n",
    "# 1) use two other classifiers\n",
    "#    1 - svc\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)\n",
    "clf.fit(iris_X_train,iris_y_train)\n",
    "clf.predict(iris_X_test)\n",
    "print \"correct score of the svc classifier : {0}\".format(accuracy_score(iris_y_test,clf.predict(iris_X_test)))\n",
    "\n",
    "#    2 - bayes\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "bayes = GaussianNB()\n",
    "bayes.fit(iris_X_train, iris_y_train)\n",
    "bayes.predict(iris_X_test)\n",
    "print \"correct score of the bayes classifier : {0}\".format(accuracy_score(iris_y_test,bayes.predict(iris_X_test)))\n",
    "\n",
    "\n",
    "#    3 - regression\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(iris_X_train,iris_y_train)\n",
    "# print regr.coef_\n",
    "# mean square error\n",
    "np.mean((regr.predict(iris_X_test)-iris_y_test)**2)\n",
    "print \"correct score of the linear regression : {0}\".format(regr.score(iris_X_test,iris_y_test))\n",
    "\n",
    "\n",
    "# 2) use cross-validation to evaluate the classifiers\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import datasets, svm\n",
    "n_samples = len(iris.data)\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "\n",
    "# cross_validation.KFold(n=6, n_folds=3) \n",
    "# n: nb elements n_folds : nb folds\n",
    "#for train_indices, test_indices in k_fold:\n",
    "#         print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "\n",
    "#    knn\n",
    "knn_cross=cross_val_score(knn,iris_X,iris_y)\n",
    "print \"result of cross-validation of knn : {0}\".format(knn_cross.mean())\n",
    "\n",
    "\n",
    "#    1 - svc\n",
    "'''\n",
    "kfold = cross_validation.KFold(n= n_samples, n_folds=5)\n",
    "[clf.fit(iris_X[train], iris_y[train]).score(iris_X[test], iris_y[test])\n",
    "             for train, test in kfold]\n",
    "svc_mean = cross_validation.cross_val_score(svc, iris_X, iris_y, cv=kfold, n_jobs=-1)\n",
    "print \"correct score of cross-validation of svc : {0}\".format(svc_mean.mean())\n",
    "'''\n",
    "svm_cross=cross_val_score(clf,iris_X,iris_y)\n",
    "print \"result of cross-validation of svm : {0}\".format(svm_cross.mean())\n",
    "\n",
    "\n",
    "\n",
    "#    2 - bayes\n",
    "'''\n",
    "kfold = cross_validation.KFold(n= n_samples, n_folds=5)\n",
    "[bayes.fit(iris_X[train], iris_y[train]).score(iris_X[test], iris_y[test])\n",
    "             for train, test in kfold]\n",
    "bayes_mean = cross_validation.cross_val_score(bayes, iris_X, iris_y, cv=kfold, n_jobs=-1)\n",
    "print \"correct score of cross-validation of bayes : {0}\".format(bayes_mean.mean())\n",
    "'''\n",
    "bayes_cross=cross_val_score(bayes,iris_X,iris_y)\n",
    "print \"result of cross-validation of bayes : {0}\".format(bayes_cross.mean())\n",
    "\n",
    "\n",
    "\n",
    "#    3 - regression\n",
    "kfold = cross_validation.KFold(n= n_samples, n_folds=5)\n",
    "[regr.fit(iris_X[train], iris_y[train]).score(iris_X[test], iris_y[test])\n",
    "             for train, test in kfold]\n",
    "regr_cross = cross_validation.cross_val_score(regr, iris_X, iris_y, cv=kfold, n_jobs=-1)\n",
    "print \"correct score of cross-validation of regression : {0}\".format(regr_cross.mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3) compariris_Xe evaluation results of the three classifiers\n",
    "# the best result is svc\n",
    "print \"the best classifiers is : knn, the result of this classifiers is : {0}\".format(knn_cross.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
